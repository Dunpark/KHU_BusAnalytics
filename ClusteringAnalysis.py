"""
================================================================================
ğŸ“Š ì„œìš¸ì‹œ ìƒê¶Œ ìœ í˜•í™” ë¶„ì„ (K-means êµ°ì§‘ ë¶„ì„)
================================================================================
- ë°ì´í„°: í•œì‹ìŒì‹ì  4ê°œë…„ í†µí•© ë°ì´í„°
- ë¶„ì„ ëª©ì : ìƒê¶Œì„ ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ìœ í˜•í™” (ì„±ì¥í˜•/ì•ˆì •í˜•/ì‡ í‡´í˜• ë“±)
- íŠ¹ì§•: ì‹œê³„ì—´ ê¸°ë°˜ íŒŒìƒë³€ìˆ˜ í¬í•¨, PCA ì°¨ì› ì¶•ì†Œ ì ìš©

ì‹¤í–‰: python ClusteringAnalysis.py
================================================================================
"""

import os
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, silhouette_samples

# ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False

# ì¶œë ¥ í´ë” ìƒì„±
OUTPUT_DIR = "ClusteringResults"
os.makedirs(OUTPUT_DIR, exist_ok=True)


def load_data():
    """
    1ë‹¨ê³„: ë°ì´í„° ë¡œë“œ
    - í•œì‹ìŒì‹ì  4ê°œë…„ í†µí•© ë°ì´í„° ë¡œë“œ
    """
    print("=" * 80)
    print("ğŸ“ 1ë‹¨ê³„: ë°ì´í„° ë¡œë“œ")
    print("=" * 80)
    
    data_path = "BrainstormingAnalytics/4ê°œë…„_í•œì‹ìŒì‹ì _í†µí•©ë°ì´í„°_ì¶”ì •ë§¤ì¶œ_ìƒì£¼ì¸êµ¬_ì†Œë“ì†Œë¹„_ê¸¸ë‹¨ìœ„ì¸êµ¬_ì í¬_ì˜ì—­.csv"
    
    df = pd.read_csv(data_path, encoding='utf-8')
    
    print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
    print(f"   - í–‰ ìˆ˜: {len(df):,}")
    print(f"   - ì—´ ìˆ˜: {df.shape[1]}")
    print(f"   - ê¸°ê°„: {df['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'].min()} ~ {df['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'].max()}")
    print(f"   - ê³ ìœ  ìƒê¶Œ ìˆ˜: {df['ìƒê¶Œ_ì½”ë“œ'].nunique():,}")
    
    return df


def extract_time_features(df):
    """
    2ë‹¨ê³„: ì‹œê³„ì—´ ê¸°ë°˜ íŒŒìƒë³€ìˆ˜ ìƒì„±
    - ë§¤ì¶œ ì„±ì¥ë¥ , ë³€ë™ì„±, ì¶”ì„¸
    """
    print("\n" + "=" * 80)
    print("ğŸ“ˆ 2ë‹¨ê³„: ì‹œê³„ì—´ ê¸°ë°˜ íŒŒìƒë³€ìˆ˜ ìƒì„±")
    print("=" * 80)
    
    # ì—°ë„ ì¶”ì¶œ
    df['ì—°ë„'] = df['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'] // 10
    df['ë¶„ê¸°'] = df['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'] % 10
    
    # ìƒê¶Œë³„ ì§‘ê³„ë¥¼ ìœ„í•œ ë¹ˆ DataFrame
    time_features = []
    
    for sangkwon_code in df['ìƒê¶Œ_ì½”ë“œ'].unique():
        sangkwon_df = df[df['ìƒê¶Œ_ì½”ë“œ'] == sangkwon_code].sort_values('ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ')
        
        # ê¸°ë³¸ ì •ë³´
        sangkwon_name = sangkwon_df['ìƒê¶Œ_ì½”ë“œ_ëª…'].iloc[0]
        area_type = sangkwon_df['ìƒê¶Œ_êµ¬ë¶„_ì½”ë“œ_ëª…'].iloc[0]
        
        # ë§¤ì¶œ ë°ì´í„°
        sales = sangkwon_df['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'].values
        quarters = sangkwon_df['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'].values
        
        # ì—°ë„ë³„ í‰ê·  ë§¤ì¶œ
        yearly_sales = sangkwon_df.groupby('ì—°ë„')['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'].mean()
        
        # ----- ì‹œê³„ì—´ íŒŒìƒë³€ìˆ˜ -----
        
        # 1. ë§¤ì¶œ ì„±ì¥ë¥  (ì²«í•´ ëŒ€ë¹„ ë§ˆì§€ë§‰í•´)
        if 2021 in yearly_sales.index and 2024 in yearly_sales.index:
            if yearly_sales[2021] > 0:
                growth_rate = (yearly_sales[2024] - yearly_sales[2021]) / yearly_sales[2021] * 100
            else:
                growth_rate = np.nan
        else:
            growth_rate = np.nan
        
        # 2. ë§¤ì¶œ ë³€ë™ì„± (ë³€ë™ê³„ìˆ˜ = í‘œì¤€í¸ì°¨/í‰ê· )
        if sales.mean() > 0:
            sales_cv = sales.std() / sales.mean() * 100
        else:
            sales_cv = np.nan
        
        # 3. ë§¤ì¶œ ì¶”ì„¸ (ì„ í˜• íšŒê·€ ê¸°ìš¸ê¸°)
        if len(sales) > 1:
            x = np.arange(len(sales))
            slope, intercept, r_value, p_value, std_err = stats.linregress(x, sales)
            # ê¸°ìš¸ê¸°ë¥¼ í‰ê·  ë§¤ì¶œ ëŒ€ë¹„ % ë³€í™”ë¡œ ì •ê·œí™”
            if sales.mean() > 0:
                sales_trend = slope / sales.mean() * 100
            else:
                sales_trend = np.nan
        else:
            sales_trend = np.nan
        
        # 4. ìµœê·¼ ë¶„ê¸° ëŒ€ë¹„ ë³€í™”ìœ¨ (ìµœê·¼ 4ë¶„ê¸° í‰ê·  vs ì´ì „ 4ë¶„ê¸° í‰ê· )
        if len(sales) >= 8:
            recent_avg = sales[-4:].mean()
            previous_avg = sales[-8:-4].mean()
            if previous_avg > 0:
                recent_change = (recent_avg - previous_avg) / previous_avg * 100
            else:
                recent_change = np.nan
        else:
            recent_change = np.nan
        
        # ----- ì •ì  ë³€ìˆ˜ (ìµœì‹  ì—°ë„ í‰ê· ) -----
        latest_year = sangkwon_df[sangkwon_df['ì—°ë„'] == 2024]
        if len(latest_year) == 0:
            latest_year = sangkwon_df[sangkwon_df['ì—°ë„'] == sangkwon_df['ì—°ë„'].max()]
        
        avg_sales = latest_year['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'].mean()
        avg_stores = latest_year['ì í¬_ìˆ˜'].mean()
        avg_floating_pop = latest_year['ì´_ìœ ë™ì¸êµ¬_ìˆ˜'].mean()
        avg_resident_pop = latest_year['ì´_ìƒì£¼ì¸êµ¬_ìˆ˜'].mean()
        avg_income = latest_year['ì›”_í‰ê· _ì†Œë“_ê¸ˆì•¡'].mean()
        avg_area = latest_year['ì˜ì—­_ë©´ì '].mean()
        
        # ì¶”ê°€ ì •ì  ë³€ìˆ˜
        avg_franchise = latest_year['í”„ëœì°¨ì´ì¦ˆ_ì í¬_ìˆ˜'].mean() if 'í”„ëœì°¨ì´ì¦ˆ_ì í¬_ìˆ˜' in latest_year.columns else 0
        avg_opening_rate = latest_year['ê°œì—…_ìœ¨'].mean() if 'ê°œì—…_ìœ¨' in latest_year.columns else 0
        avg_closing_rate = latest_year['íì—…_ë¥ '].mean() if 'íì—…_ë¥ ' in latest_year.columns else 0
        
        # ì‹œê°„ëŒ€ë³„ ë§¤ì¶œ ë¹„ìœ¨ (ë™ì  íŒ¨í„´)
        total_sales = latest_year['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'].sum()
        if total_sales > 0:
            lunch_ratio = latest_year['ì‹œê°„ëŒ€_11~14_ë§¤ì¶œ_ê¸ˆì•¡'].sum() / total_sales * 100
            dinner_ratio = latest_year['ì‹œê°„ëŒ€_17~21_ë§¤ì¶œ_ê¸ˆì•¡'].sum() / total_sales * 100
            night_ratio = latest_year['ì‹œê°„ëŒ€_21~24_ë§¤ì¶œ_ê¸ˆì•¡'].sum() / total_sales * 100
        else:
            lunch_ratio = dinner_ratio = night_ratio = np.nan
        
        # ì£¼ì¤‘/ì£¼ë§ ë§¤ì¶œ ë¹„ìœ¨
        weekday_sales = latest_year['ì£¼ì¤‘_ë§¤ì¶œ_ê¸ˆì•¡'].sum()
        weekend_sales = latest_year['ì£¼ë§_ë§¤ì¶œ_ê¸ˆì•¡'].sum()
        if (weekday_sales + weekend_sales) > 0:
            weekend_ratio = weekend_sales / (weekday_sales + weekend_sales) * 100
        else:
            weekend_ratio = np.nan
        
        time_features.append({
            'ìƒê¶Œ_ì½”ë“œ': sangkwon_code,
            'ìƒê¶Œ_ì½”ë“œ_ëª…': sangkwon_name,
            'ìƒê¶Œ_êµ¬ë¶„_ì½”ë“œ_ëª…': area_type,
            
            # ì‹œê³„ì—´ íŒŒìƒë³€ìˆ˜
            'ë§¤ì¶œ_ì„±ì¥ë¥ ': growth_rate,
            'ë§¤ì¶œ_ë³€ë™ì„±': sales_cv,
            'ë§¤ì¶œ_ì¶”ì„¸': sales_trend,
            'ìµœê·¼_ë³€í™”ìœ¨': recent_change,
            
            # ì •ì  ë³€ìˆ˜
            'í‰ê· _ë§¤ì¶œ': avg_sales,
            'ì í¬_ìˆ˜': avg_stores,
            'ì´_ìœ ë™ì¸êµ¬_ìˆ˜': avg_floating_pop,
            'ì´_ìƒì£¼ì¸êµ¬_ìˆ˜': avg_resident_pop,
            'ì›”_í‰ê· _ì†Œë“_ê¸ˆì•¡': avg_income,
            'ì˜ì—­_ë©´ì ': avg_area,
            'í”„ëœì°¨ì´ì¦ˆ_ì í¬_ìˆ˜': avg_franchise,
            'ê°œì—…_ìœ¨': avg_opening_rate,
            'íì—…_ë¥ ': avg_closing_rate,
            
            # ì‹œê°„ëŒ€/ìš”ì¼ íŒ¨í„´
            'ì ì‹¬_ë§¤ì¶œ_ë¹„ìœ¨': lunch_ratio,
            'ì €ë…_ë§¤ì¶œ_ë¹„ìœ¨': dinner_ratio,
            'ì•¼ê°„_ë§¤ì¶œ_ë¹„ìœ¨': night_ratio,
            'ì£¼ë§_ë§¤ì¶œ_ë¹„ìœ¨': weekend_ratio,
        })
    
    features_df = pd.DataFrame(time_features)
    
    print(f"âœ… ì‹œê³„ì—´ íŒŒìƒë³€ìˆ˜ ìƒì„± ì™„ë£Œ")
    print(f"   - ìƒê¶Œ ìˆ˜: {len(features_df):,}")
    print(f"   - ë³€ìˆ˜ ìˆ˜: {features_df.shape[1]}")
    
    # ê²°ì¸¡ì¹˜ í˜„í™©
    print("\nğŸ“Š íŒŒìƒë³€ìˆ˜ ê²°ì¸¡ì¹˜ í˜„í™©:")
    missing = features_df.isnull().sum()
    for col in missing[missing > 0].index:
        print(f"   - {col}: {missing[col]} ({missing[col]/len(features_df)*100:.1f}%)")
    
    return features_df


def preprocess_features(features_df):
    """
    3ë‹¨ê³„: ë³€ìˆ˜ ì„ íƒ ë° ì „ì²˜ë¦¬
    - ë¶„ì„ì— ì‚¬ìš©í•  ë³€ìˆ˜ ì„ íƒ
    - ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    - ì´ìƒì¹˜ ì²˜ë¦¬
    """
    print("\n" + "=" * 80)
    print("ğŸ”§ 3ë‹¨ê³„: ë³€ìˆ˜ ì„ íƒ ë° ì „ì²˜ë¦¬")
    print("=" * 80)
    
    # ë¶„ì„ì— ì‚¬ìš©í•  ë³€ìˆ˜ ì„ íƒ
    feature_cols = [
        # ì‹œê³„ì—´ íŒŒìƒë³€ìˆ˜
        'ë§¤ì¶œ_ì„±ì¥ë¥ ',
        'ë§¤ì¶œ_ë³€ë™ì„±',
        'ë§¤ì¶œ_ì¶”ì„¸',
        
        # ì •ì  ë³€ìˆ˜ (ë¡œê·¸ ë³€í™˜ ëŒ€ìƒ)
        'í‰ê· _ë§¤ì¶œ',
        'ì í¬_ìˆ˜',
        'ì´_ìœ ë™ì¸êµ¬_ìˆ˜',
        'ì´_ìƒì£¼ì¸êµ¬_ìˆ˜',
        'ì›”_í‰ê· _ì†Œë“_ê¸ˆì•¡',
        'ì˜ì—­_ë©´ì ',
        
        # ì‹œê°„ëŒ€/ìš”ì¼ íŒ¨í„´
        'ì ì‹¬_ë§¤ì¶œ_ë¹„ìœ¨',
        'ì €ë…_ë§¤ì¶œ_ë¹„ìœ¨',
        'ì£¼ë§_ë§¤ì¶œ_ë¹„ìœ¨',
    ]
    
    df_analysis = features_df[['ìƒê¶Œ_ì½”ë“œ', 'ìƒê¶Œ_ì½”ë“œ_ëª…', 'ìƒê¶Œ_êµ¬ë¶„_ì½”ë“œ_ëª…'] + feature_cols].copy()
    
    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (ì¤‘ì•™ê°’ ëŒ€ì²´)
    print("\nğŸ“ ê²°ì¸¡ì¹˜ ì²˜ë¦¬:")
    for col in feature_cols:
        n_missing = df_analysis[col].isnull().sum()
        if n_missing > 0:
            median_val = df_analysis[col].median()
            df_analysis[col].fillna(median_val, inplace=True)
            print(f"   - {col}: {n_missing}ê°œ â†’ ì¤‘ì•™ê°’({median_val:.2f})ìœ¼ë¡œ ëŒ€ì²´")
    
    # ë¡œê·¸ ë³€í™˜ (ìš°í¸í–¥ ë³€ìˆ˜)
    log_cols = ['í‰ê· _ë§¤ì¶œ', 'ì í¬_ìˆ˜', 'ì´_ìœ ë™ì¸êµ¬_ìˆ˜', 'ì´_ìƒì£¼ì¸êµ¬_ìˆ˜', 'ì˜ì—­_ë©´ì ']
    print("\nğŸ“ ë¡œê·¸ ë³€í™˜:")
    for col in log_cols:
        original_skew = df_analysis[col].skew()
        df_analysis[f'{col}_log'] = np.log1p(df_analysis[col])
        new_skew = df_analysis[f'{col}_log'].skew()
        print(f"   - {col}: ì™œë„ {original_skew:.2f} â†’ {new_skew:.2f}")
    
    # ìµœì¢… ë¶„ì„ ë³€ìˆ˜ ì„ íƒ
    final_feature_cols = [
        'ë§¤ì¶œ_ì„±ì¥ë¥ ',
        'ë§¤ì¶œ_ë³€ë™ì„±',
        'ë§¤ì¶œ_ì¶”ì„¸',
        'í‰ê· _ë§¤ì¶œ_log',
        'ì í¬_ìˆ˜_log',
        'ì´_ìœ ë™ì¸êµ¬_ìˆ˜_log',
        'ì´_ìƒì£¼ì¸êµ¬_ìˆ˜_log',
        'ì›”_í‰ê· _ì†Œë“_ê¸ˆì•¡',
        'ì˜ì—­_ë©´ì _log',
        'ì ì‹¬_ë§¤ì¶œ_ë¹„ìœ¨',
        'ì €ë…_ë§¤ì¶œ_ë¹„ìœ¨',
        'ì£¼ë§_ë§¤ì¶œ_ë¹„ìœ¨',
    ]
    
    print(f"\nâœ… ì „ì²˜ë¦¬ ì™„ë£Œ")
    print(f"   - ìµœì¢… ë¶„ì„ ë³€ìˆ˜: {len(final_feature_cols)}ê°œ")
    for col in final_feature_cols:
        print(f"      â€¢ {col}")
    
    return df_analysis, final_feature_cols


def standardize_and_pca(df_analysis, feature_cols):
    """
    4ë‹¨ê³„: í‘œì¤€í™” ë° PCA ì°¨ì› ì¶•ì†Œ
    - z-score í‘œì¤€í™”
    - PCAë¡œ í•µì‹¬ ìš”ì¸ ì¶”ì¶œ (ì„ í–‰ì—°êµ¬ Factor Analysis ì°¸ê³ )
    """
    print("\n" + "=" * 80)
    print("ğŸ“ 4ë‹¨ê³„: í‘œì¤€í™” ë° PCA ì°¨ì› ì¶•ì†Œ")
    print("=" * 80)
    
    X = df_analysis[feature_cols].values
    
    # z-score í‘œì¤€í™”
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    print("âœ… z-score í‘œì¤€í™” ì™„ë£Œ")
    
    # PCA ìˆ˜í–‰ (ì„¤ëª…ë ¥ 95% ì´ìƒ)
    pca_full = PCA()
    pca_full.fit(X_scaled)
    
    # ëˆ„ì  ì„¤ëª…ë ¥ ê³„ì‚°
    cumsum_var = np.cumsum(pca_full.explained_variance_ratio_)
    n_components = np.argmax(cumsum_var >= 0.90) + 1  # 90% ì„¤ëª…ë ¥
    
    print(f"\nğŸ“Š PCA ë¶„ì„ ê²°ê³¼:")
    print(f"   - ì›ë³¸ ë³€ìˆ˜ ìˆ˜: {len(feature_cols)}")
    print(f"   - 90% ì„¤ëª…ë ¥ ë‹¬ì„± ì£¼ì„±ë¶„ ìˆ˜: {n_components}")
    
    # ì£¼ì„±ë¶„ë³„ ì„¤ëª…ë ¥
    print("\n   ì£¼ì„±ë¶„ë³„ ì„¤ëª…ë ¥:")
    for i, var in enumerate(pca_full.explained_variance_ratio_[:n_components+2]):
        cum_var = cumsum_var[i]
        print(f"      PC{i+1}: {var*100:.1f}% (ëˆ„ì : {cum_var*100:.1f}%)")
    
    # ìµœì¢… PCA ì ìš©
    pca = PCA(n_components=n_components)
    X_pca = pca.fit_transform(X_scaled)
    
    # ì£¼ì„±ë¶„ í•´ì„ (ë¡œë”©)
    print("\nğŸ“Š ì£¼ì„±ë¶„ í•´ì„ (ì£¼ìš” ë¡œë”©):")
    loadings = pd.DataFrame(
        pca.components_.T,
        columns=[f'PC{i+1}' for i in range(n_components)],
        index=feature_cols
    )
    
    for i in range(min(3, n_components)):
        pc = f'PC{i+1}'
        print(f"\n   {pc} (ì„¤ëª…ë ¥: {pca.explained_variance_ratio_[i]*100:.1f}%):")
        top_loadings = loadings[pc].abs().sort_values(ascending=False).head(3)
        for var in top_loadings.index:
            val = loadings.loc[var, pc]
            print(f"      â€¢ {var}: {val:.3f}")
    
    # ì‹œê°í™”: ì„¤ëª…ë ¥ ìŠ¤í¬ë¦¬ í”Œë¡¯
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # ê°œë³„ ì„¤ëª…ë ¥
    axes[0].bar(range(1, len(pca_full.explained_variance_ratio_)+1), 
                pca_full.explained_variance_ratio_ * 100, color='steelblue', alpha=0.7)
    axes[0].axhline(y=10, color='red', linestyle='--', label='10% ê¸°ì¤€ì„ ')
    axes[0].set_xlabel('ì£¼ì„±ë¶„')
    axes[0].set_ylabel('ì„¤ëª…ë ¥ (%)')
    axes[0].set_title('ì£¼ì„±ë¶„ë³„ ì„¤ëª…ë ¥ (Scree Plot)')
    axes[0].legend()
    
    # ëˆ„ì  ì„¤ëª…ë ¥
    axes[1].plot(range(1, len(cumsum_var)+1), cumsum_var * 100, 'o-', color='steelblue')
    axes[1].axhline(y=90, color='red', linestyle='--', label='90% ê¸°ì¤€ì„ ')
    axes[1].axvline(x=n_components, color='green', linestyle='--', label=f'ì„ íƒ ì£¼ì„±ë¶„ ({n_components}ê°œ)')
    axes[1].set_xlabel('ì£¼ì„±ë¶„ ìˆ˜')
    axes[1].set_ylabel('ëˆ„ì  ì„¤ëª…ë ¥ (%)')
    axes[1].set_title('ëˆ„ì  ì„¤ëª…ë ¥')
    axes[1].legend()
    
    plt.tight_layout()
    plt.savefig(f"{OUTPUT_DIR}/01_pca_scree_plot.png", dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"\nâœ… PCA ì°¨ì› ì¶•ì†Œ ì™„ë£Œ: {len(feature_cols)}ê°œ â†’ {n_components}ê°œ ì£¼ì„±ë¶„")
    
    return X_scaled, X_pca, pca, scaler, loadings


def find_optimal_k(X, max_k=10):
    """
    5-1ë‹¨ê³„: ìµœì  k íƒìƒ‰
    - Elbow Method
    - Silhouette Score
    """
    print("\n" + "=" * 80)
    print("ğŸ” 5-1ë‹¨ê³„: ìµœì  k íƒìƒ‰")
    print("=" * 80)
    
    inertias = []
    silhouettes = []
    K_range = range(2, max_k + 1)
    
    for k in K_range:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        kmeans.fit(X)
        inertias.append(kmeans.inertia_)
        silhouettes.append(silhouette_score(X, kmeans.labels_))
    
    # ì‹œê°í™”
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # Elbow Plot
    axes[0].plot(K_range, inertias, 'o-', color='steelblue', linewidth=2, markersize=8)
    axes[0].set_xlabel('í´ëŸ¬ìŠ¤í„° ìˆ˜ (k)')
    axes[0].set_ylabel('Inertia (Within-cluster Sum of Squares)')
    axes[0].set_title('Elbow Method')
    axes[0].set_xticks(list(K_range))
    
    # Silhouette Score
    axes[1].plot(K_range, silhouettes, 'o-', color='coral', linewidth=2, markersize=8)
    axes[1].set_xlabel('í´ëŸ¬ìŠ¤í„° ìˆ˜ (k)')
    axes[1].set_ylabel('Silhouette Score')
    axes[1].set_title('Silhouette Score by k')
    axes[1].set_xticks(list(K_range))
    axes[1].axhline(y=0.3, color='red', linestyle='--', label='ìµœì†Œ ê¸°ì¤€ (0.3)')
    axes[1].legend()
    
    plt.tight_layout()
    plt.savefig(f"{OUTPUT_DIR}/02_optimal_k_search.png", dpi=150, bbox_inches='tight')
    plt.close()
    
    # ìµœì  k ê²°ì • (ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ ê¸°ì¤€)
    best_k = K_range[np.argmax(silhouettes)]
    best_silhouette = max(silhouettes)
    
    print("ğŸ“Š kë³„ ì„±ëŠ¥:")
    for k, (inertia, sil) in enumerate(zip(inertias, silhouettes), start=2):
        marker = " â˜…" if k == best_k else ""
        print(f"   k={k}: Inertia={inertia:.0f}, Silhouette={sil:.3f}{marker}")
    
    print(f"\nâœ… ìµœì  k ê²°ì •: {best_k} (Silhouette Score: {best_silhouette:.3f})")
    
    return best_k, silhouettes


def perform_kmeans(X, k):
    """
    5-2ë‹¨ê³„: K-means í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
    """
    print("\n" + "=" * 80)
    print(f"ğŸ¯ 5-2ë‹¨ê³„: K-means í´ëŸ¬ìŠ¤í„°ë§ (k={k})")
    print("=" * 80)
    
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = kmeans.fit_predict(X)
    
    # í´ëŸ¬ìŠ¤í„°ë³„ ê°œìˆ˜
    unique, counts = np.unique(labels, return_counts=True)
    
    print("âœ… í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ")
    print("\nğŸ“Š í´ëŸ¬ìŠ¤í„°ë³„ ìƒê¶Œ ìˆ˜:")
    for cluster, count in zip(unique, counts):
        print(f"   - í´ëŸ¬ìŠ¤í„° {cluster}: {count}ê°œ ({count/len(labels)*100:.1f}%)")
    
    # ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´
    overall_silhouette = silhouette_score(X, labels)
    sample_silhouettes = silhouette_samples(X, labels)
    
    print(f"\nğŸ“Š ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´:")
    print(f"   - ì „ì²´: {overall_silhouette:.3f}")
    for cluster in unique:
        cluster_silhouette = sample_silhouettes[labels == cluster].mean()
        print(f"   - í´ëŸ¬ìŠ¤í„° {cluster}: {cluster_silhouette:.3f}")
    
    return kmeans, labels


def interpret_clusters(df_analysis, feature_cols, labels, original_feature_cols):
    """
    6ë‹¨ê³„: í´ëŸ¬ìŠ¤í„° í•´ì„
    - Centroid ë¶„ì„
    - í´ëŸ¬ìŠ¤í„° í”„ë¡œíŒŒì¼ë§
    """
    print("\n" + "=" * 80)
    print("ğŸ“– 6ë‹¨ê³„: í´ëŸ¬ìŠ¤í„° í•´ì„ ë° í”„ë¡œíŒŒì¼ë§")
    print("=" * 80)
    
    df_result = df_analysis.copy()
    df_result['í´ëŸ¬ìŠ¤í„°'] = labels
    
    # í´ëŸ¬ìŠ¤í„°ë³„ í†µê³„
    print("\nğŸ“Š í´ëŸ¬ìŠ¤í„°ë³„ íŠ¹ì„± (í‰ê· ê°’):")
    
    # ì›ë³¸ ë³€ìˆ˜ ê¸°ì¤€ í†µê³„
    analysis_cols = [
        'í‰ê· _ë§¤ì¶œ', 'ë§¤ì¶œ_ì„±ì¥ë¥ ', 'ë§¤ì¶œ_ë³€ë™ì„±', 'ë§¤ì¶œ_ì¶”ì„¸',
        'ì í¬_ìˆ˜', 'ì´_ìœ ë™ì¸êµ¬_ìˆ˜', 'ì´_ìƒì£¼ì¸êµ¬_ìˆ˜', 'ì›”_í‰ê· _ì†Œë“_ê¸ˆì•¡',
        'ì ì‹¬_ë§¤ì¶œ_ë¹„ìœ¨', 'ì €ë…_ë§¤ì¶œ_ë¹„ìœ¨', 'ì£¼ë§_ë§¤ì¶œ_ë¹„ìœ¨'
    ]
    
    cluster_stats = df_result.groupby('í´ëŸ¬ìŠ¤í„°')[analysis_cols].mean()
    
    # ì „ì²´ í‰ê·  ëŒ€ë¹„ ë¹„ìœ¨ë¡œ í•´ì„
    overall_mean = df_result[analysis_cols].mean()
    cluster_relative = cluster_stats / overall_mean * 100 - 100  # % ì°¨ì´
    
    print("\n" + "-" * 80)
    print("í´ëŸ¬ìŠ¤í„°ë³„ í‰ê· ê°’ (ì „ì²´ í‰ê·  ëŒ€ë¹„ % ì°¨ì´)")
    print("-" * 80)
    
    for cluster in sorted(df_result['í´ëŸ¬ìŠ¤í„°'].unique()):
        n_samples = (df_result['í´ëŸ¬ìŠ¤í„°'] == cluster).sum()
        print(f"\nğŸ·ï¸ í´ëŸ¬ìŠ¤í„° {cluster} (n={n_samples})")
        
        for col in analysis_cols:
            val = cluster_stats.loc[cluster, col]
            rel_diff = cluster_relative.loc[cluster, col]
            
            # ë‹¨ìœ„ í¬ë§·íŒ…
            if 'ë§¤ì¶œ' in col and 'ë¹„ìœ¨' not in col and 'ì„±ì¥ë¥ ' not in col and 'ë³€ë™ì„±' not in col and 'ì¶”ì„¸' not in col:
                val_str = f"{val/1e8:.1f}ì–µì›"
            elif 'ì¸êµ¬' in col:
                val_str = f"{val/1e4:.1f}ë§Œëª…"
            elif 'ì†Œë“' in col:
                val_str = f"{val/1e4:.0f}ë§Œì›"
            elif 'ë¹„ìœ¨' in col or 'ì„±ì¥ë¥ ' in col:
                val_str = f"{val:.1f}%"
            else:
                val_str = f"{val:.1f}"
            
            # ë°©í–¥ í‘œì‹œ
            if rel_diff > 20:
                direction = "â–²â–²"
            elif rel_diff > 5:
                direction = "â–²"
            elif rel_diff < -20:
                direction = "â–¼â–¼"
            elif rel_diff < -5:
                direction = "â–¼"
            else:
                direction = "â”"
            
            print(f"   {col}: {val_str} ({direction} {rel_diff:+.1f}%)")
    
    # í´ëŸ¬ìŠ¤í„° ë„¤ì´ë°
    print("\n" + "=" * 80)
    print("ğŸ·ï¸ í´ëŸ¬ìŠ¤í„° í”„ë¡œíŒŒì¼ ë° ë„¤ì´ë° ì œì•ˆ")
    print("=" * 80)
    
    cluster_profiles = {}
    for cluster in sorted(df_result['í´ëŸ¬ìŠ¤í„°'].unique()):
        growth = cluster_stats.loc[cluster, 'ë§¤ì¶œ_ì„±ì¥ë¥ ']
        sales = cluster_stats.loc[cluster, 'í‰ê· _ë§¤ì¶œ']
        volatility = cluster_stats.loc[cluster, 'ë§¤ì¶œ_ë³€ë™ì„±']
        trend = cluster_stats.loc[cluster, 'ë§¤ì¶œ_ì¶”ì„¸']
        
        # íŠ¹ì„± ê¸°ë°˜ ë¶„ë¥˜
        characteristics = []
        
        # ì„±ì¥ì„±
        if growth > 20:
            characteristics.append("ê³ ì„±ì¥")
        elif growth > 0:
            characteristics.append("ì•ˆì •ì„±ì¥")
        elif growth > -10:
            characteristics.append("ì •ì²´")
        else:
            characteristics.append("ì‡ í‡´")
        
        # ê·œëª¨
        overall_sales = df_result['í‰ê· _ë§¤ì¶œ'].median()
        if sales > overall_sales * 2:
            characteristics.append("ëŒ€í˜•")
        elif sales > overall_sales:
            characteristics.append("ì¤‘í˜•")
        else:
            characteristics.append("ì†Œí˜•")
        
        # ì•ˆì •ì„±
        if volatility < 30:
            characteristics.append("ì•ˆì •")
        elif volatility > 50:
            characteristics.append("ë³€ë™")
        
        profile_name = " ".join(characteristics) + " ìƒê¶Œ"
        cluster_profiles[cluster] = profile_name
        
        print(f"\ní´ëŸ¬ìŠ¤í„° {cluster}: ã€Œ{profile_name}ã€")
        print(f"   - ì„±ì¥ë¥ : {growth:.1f}%")
        print(f"   - í‰ê· ë§¤ì¶œ: {sales/1e8:.1f}ì–µì›")
        print(f"   - ë³€ë™ì„±: {volatility:.1f}%")
        print(f"   - ì¶”ì„¸: {trend:.2f}")
    
    df_result['í´ëŸ¬ìŠ¤í„°_ëª…'] = df_result['í´ëŸ¬ìŠ¤í„°'].map(cluster_profiles)
    
    return df_result, cluster_stats, cluster_profiles


def visualize_clusters(df_result, X_pca, labels, cluster_profiles):
    """
    7ë‹¨ê³„: í´ëŸ¬ìŠ¤í„° ì‹œê°í™”
    """
    print("\n" + "=" * 80)
    print("ğŸ“Š 7ë‹¨ê³„: í´ëŸ¬ìŠ¤í„° ì‹œê°í™”")
    print("=" * 80)
    
    n_clusters = len(cluster_profiles)
    colors = plt.cm.Set1(np.linspace(0, 1, n_clusters))
    
    # 1. PCA 2D ì‚°ì ë„
    fig, ax = plt.subplots(figsize=(12, 8))
    
    for cluster in sorted(cluster_profiles.keys()):
        mask = labels == cluster
        ax.scatter(X_pca[mask, 0], X_pca[mask, 1], 
                   label=f'{cluster}: {cluster_profiles[cluster]}',
                   alpha=0.6, s=50, color=colors[cluster])
    
    ax.set_xlabel('PC1')
    ax.set_ylabel('PC2')
    ax.set_title('ìƒê¶Œ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ (PCA 2D íˆ¬ì˜)')
    ax.legend(bbox_to_anchor=(1.02, 1), loc='upper left')
    
    plt.tight_layout()
    plt.savefig(f"{OUTPUT_DIR}/03_cluster_pca_scatter.png", dpi=150, bbox_inches='tight')
    plt.close()
    
    # 2. í´ëŸ¬ìŠ¤í„°ë³„ ì£¼ìš” ì§€í‘œ ë°•ìŠ¤í”Œë¡¯
    key_features = ['í‰ê· _ë§¤ì¶œ', 'ë§¤ì¶œ_ì„±ì¥ë¥ ', 'ë§¤ì¶œ_ë³€ë™ì„±', 'ì´_ìœ ë™ì¸êµ¬_ìˆ˜']
    
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    axes = axes.flatten()
    
    for i, feature in enumerate(key_features):
        data_to_plot = []
        cluster_labels = []
        
        for cluster in sorted(cluster_profiles.keys()):
            data = df_result[df_result['í´ëŸ¬ìŠ¤í„°'] == cluster][feature].values
            data_to_plot.append(data)
            cluster_labels.append(f'{cluster}: {cluster_profiles[cluster][:6]}')
        
        bp = axes[i].boxplot(data_to_plot, labels=cluster_labels, patch_artist=True)
        
        for patch, color in zip(bp['boxes'], colors):
            patch.set_facecolor(color)
            patch.set_alpha(0.6)
        
        axes[i].set_title(feature)
        axes[i].tick_params(axis='x', rotation=45)
    
    plt.tight_layout()
    plt.savefig(f"{OUTPUT_DIR}/04_cluster_boxplot.png", dpi=150, bbox_inches='tight')
    plt.close()
    
    # 3. í´ëŸ¬ìŠ¤í„°ë³„ ë ˆì´ë” ì°¨íŠ¸
    categories = ['ë§¤ì¶œ_ì„±ì¥ë¥ ', 'ë§¤ì¶œ_ë³€ë™ì„±', 'ì í¬_ìˆ˜', 'ì´_ìœ ë™ì¸êµ¬_ìˆ˜', 'ì›”_í‰ê· _ì†Œë“_ê¸ˆì•¡', 'ì£¼ë§_ë§¤ì¶œ_ë¹„ìœ¨']
    
    # ì •ê·œí™” (0-1)
    normalized_stats = df_result.groupby('í´ëŸ¬ìŠ¤í„°')[categories].mean()
    for col in categories:
        min_val = normalized_stats[col].min()
        max_val = normalized_stats[col].max()
        if max_val > min_val:
            normalized_stats[col] = (normalized_stats[col] - min_val) / (max_val - min_val)
        else:
            normalized_stats[col] = 0.5
    
    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))
    
    angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
    angles += angles[:1]
    
    for cluster in sorted(cluster_profiles.keys()):
        values = normalized_stats.loc[cluster].tolist()
        values += values[:1]
        ax.plot(angles, values, 'o-', linewidth=2, label=f'{cluster}: {cluster_profiles[cluster]}', color=colors[cluster])
        ax.fill(angles, values, alpha=0.15, color=colors[cluster])
    
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(categories)
    ax.set_title('í´ëŸ¬ìŠ¤í„°ë³„ íŠ¹ì„± í”„ë¡œíŒŒì¼ (ë ˆì´ë” ì°¨íŠ¸)')
    ax.legend(bbox_to_anchor=(1.1, 1), loc='upper left')
    
    plt.tight_layout()
    plt.savefig(f"{OUTPUT_DIR}/05_cluster_radar.png", dpi=150, bbox_inches='tight')
    plt.close()
    
    # 4. ìƒê¶Œ ìœ í˜•(ê³¨ëª©/ë°œë‹¬/ì „í†µì‹œì¥) vs í´ëŸ¬ìŠ¤í„° êµì°¨í‘œ
    if 'ìƒê¶Œ_êµ¬ë¶„_ì½”ë“œ_ëª…' in df_result.columns:
        crosstab = pd.crosstab(df_result['ìƒê¶Œ_êµ¬ë¶„_ì½”ë“œ_ëª…'], df_result['í´ëŸ¬ìŠ¤í„°_ëª…'], normalize='index') * 100
        
        fig, ax = plt.subplots(figsize=(12, 6))
        crosstab.plot(kind='bar', stacked=True, ax=ax, colormap='Set2')
        ax.set_xlabel('ê¸°ì¡´ ìƒê¶Œ ìœ í˜•')
        ax.set_ylabel('ë¹„ìœ¨ (%)')
        ax.set_title('ê¸°ì¡´ ìƒê¶Œ ìœ í˜•ë³„ í´ëŸ¬ìŠ¤í„° ë¶„í¬')
        ax.legend(title='í´ëŸ¬ìŠ¤í„°', bbox_to_anchor=(1.02, 1), loc='upper left')
        ax.tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        plt.savefig(f"{OUTPUT_DIR}/06_cluster_vs_area_type.png", dpi=150, bbox_inches='tight')
        plt.close()
    
    print("âœ… ì‹œê°í™” ì™„ë£Œ")
    print(f"   - ì €ì¥ ìœ„ì¹˜: {OUTPUT_DIR}/")


def save_results(df_result, cluster_stats, cluster_profiles):
    """
    8ë‹¨ê³„: ê²°ê³¼ ì €ì¥
    """
    print("\n" + "=" * 80)
    print("ğŸ’¾ 8ë‹¨ê³„: ê²°ê³¼ ì €ì¥")
    print("=" * 80)
    
    # 1. ìƒê¶Œë³„ í´ëŸ¬ìŠ¤í„° ê²°ê³¼
    output_cols = ['ìƒê¶Œ_ì½”ë“œ', 'ìƒê¶Œ_ì½”ë“œ_ëª…', 'ìƒê¶Œ_êµ¬ë¶„_ì½”ë“œ_ëª…', 'í´ëŸ¬ìŠ¤í„°', 'í´ëŸ¬ìŠ¤í„°_ëª…',
                   'í‰ê· _ë§¤ì¶œ', 'ë§¤ì¶œ_ì„±ì¥ë¥ ', 'ë§¤ì¶œ_ë³€ë™ì„±', 'ë§¤ì¶œ_ì¶”ì„¸',
                   'ì í¬_ìˆ˜', 'ì´_ìœ ë™ì¸êµ¬_ìˆ˜', 'ì›”_í‰ê· _ì†Œë“_ê¸ˆì•¡']
    
    df_result[output_cols].to_csv(f"{OUTPUT_DIR}/clustering_result.csv", index=False, encoding='utf-8-sig')
    
    # 2. í´ëŸ¬ìŠ¤í„° í†µê³„
    cluster_stats.to_csv(f"{OUTPUT_DIR}/cluster_statistics.csv", encoding='utf-8-sig')
    
    # 3. í´ëŸ¬ìŠ¤í„° í”„ë¡œíŒŒì¼ ìš”ì•½
    profile_df = pd.DataFrame([
        {'í´ëŸ¬ìŠ¤í„°': k, 'í”„ë¡œíŒŒì¼': v, 'ìƒê¶Œìˆ˜': (df_result['í´ëŸ¬ìŠ¤í„°'] == k).sum()}
        for k, v in cluster_profiles.items()
    ])
    profile_df.to_csv(f"{OUTPUT_DIR}/cluster_profiles.csv", index=False, encoding='utf-8-sig')
    
    print("âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ")
    print(f"   - {OUTPUT_DIR}/clustering_result.csv")
    print(f"   - {OUTPUT_DIR}/cluster_statistics.csv")
    print(f"   - {OUTPUT_DIR}/cluster_profiles.csv")


def generate_business_insights(df_result, cluster_profiles, cluster_stats):
    """
    9ë‹¨ê³„: ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ
    """
    print("\n" + "=" * 80)
    print("ğŸ’¡ 9ë‹¨ê³„: ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸")
    print("=" * 80)
    
    print("\nğŸ“Œ ì£¼ìš” ë°œê²¬ ì‚¬í•­:")
    
    # ì„±ì¥ë¥  ê¸°ì¤€ ë¶„ë¥˜
    growth_clusters = cluster_stats['ë§¤ì¶œ_ì„±ì¥ë¥ '].sort_values(ascending=False)
    best_growth = growth_clusters.index[0]
    worst_growth = growth_clusters.index[-1]
    
    print(f"\n1ï¸âƒ£ ê°€ì¥ ì„±ì¥í•œ ìƒê¶Œ ìœ í˜•: í´ëŸ¬ìŠ¤í„° {best_growth} ({cluster_profiles[best_growth]})")
    print(f"   - ë§¤ì¶œ ì„±ì¥ë¥ : {cluster_stats.loc[best_growth, 'ë§¤ì¶œ_ì„±ì¥ë¥ ']:.1f}%")
    print(f"   - ìƒê¶Œ ìˆ˜: {(df_result['í´ëŸ¬ìŠ¤í„°'] == best_growth).sum()}ê°œ")
    
    print(f"\n2ï¸âƒ£ ê°€ì¥ ì‡ í‡´í•œ ìƒê¶Œ ìœ í˜•: í´ëŸ¬ìŠ¤í„° {worst_growth} ({cluster_profiles[worst_growth]})")
    print(f"   - ë§¤ì¶œ ì„±ì¥ë¥ : {cluster_stats.loc[worst_growth, 'ë§¤ì¶œ_ì„±ì¥ë¥ ']:.1f}%")
    print(f"   - ìƒê¶Œ ìˆ˜: {(df_result['í´ëŸ¬ìŠ¤í„°'] == worst_growth).sum()}ê°œ")
    
    # ê°€ì¥ í° í´ëŸ¬ìŠ¤í„°
    largest_cluster = df_result['í´ëŸ¬ìŠ¤í„°'].value_counts().idxmax()
    print(f"\n3ï¸âƒ£ ê°€ì¥ ë§ì€ ìƒê¶Œ ìœ í˜•: í´ëŸ¬ìŠ¤í„° {largest_cluster} ({cluster_profiles[largest_cluster]})")
    print(f"   - ìƒê¶Œ ìˆ˜: {(df_result['í´ëŸ¬ìŠ¤í„°'] == largest_cluster).sum()}ê°œ")
    
    print("\nğŸ“Œ ì˜ì‚¬ê²°ì • ì‹œì‚¬ì :")
    print("\n[ì°½ì—…ì ê´€ì ]")
    print(f"   â€¢ ì„±ì¥ ì ì¬ë ¥ì´ ë†’ì€ ã€Œ{cluster_profiles[best_growth]}ã€ ìœ í˜• ìƒê¶Œ ìš°ì„  ê²€í† ")
    print(f"   â€¢ ã€Œ{cluster_profiles[worst_growth]}ã€ ìœ í˜•ì€ ì‹ ì¤‘í•œ ì§„ì… í•„ìš”")
    
    print("\n[ì •ì±… ê´€ì ]")
    print(f"   â€¢ ã€Œ{cluster_profiles[worst_growth]}ã€ ìœ í˜• ìƒê¶Œì— ëŒ€í•œ í™œì„±í™” ì •ì±… í•„ìš”")
    print("   â€¢ í´ëŸ¬ìŠ¤í„°ë³„ ë§ì¶¤í˜• ì§€ì› ì „ëµ ìˆ˜ë¦½ ê°€ëŠ¥")
    
    # ê²°ê³¼ íŒŒì¼ë¡œ ì €ì¥
    with open(f"{OUTPUT_DIR}/business_insights.txt", 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("ì„œìš¸ì‹œ í•œì‹ìŒì‹ì  ìƒê¶Œ ìœ í˜•í™” ë¶„ì„ - ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸\n")
        f.write("=" * 80 + "\n\n")
        
        f.write("ğŸ“Œ í´ëŸ¬ìŠ¤í„° í”„ë¡œíŒŒì¼ ìš”ì•½\n")
        f.write("-" * 40 + "\n")
        for cluster, profile in cluster_profiles.items():
            n = (df_result['í´ëŸ¬ìŠ¤í„°'] == cluster).sum()
            growth = cluster_stats.loc[cluster, 'ë§¤ì¶œ_ì„±ì¥ë¥ ']
            sales = cluster_stats.loc[cluster, 'í‰ê· _ë§¤ì¶œ'] / 1e8
            f.write(f"í´ëŸ¬ìŠ¤í„° {cluster}: {profile}\n")
            f.write(f"  - ìƒê¶Œ ìˆ˜: {n}ê°œ\n")
            f.write(f"  - í‰ê·  ë§¤ì¶œ: {sales:.1f}ì–µì›\n")
            f.write(f"  - ì„±ì¥ë¥ : {growth:.1f}%\n\n")
        
        f.write("\nğŸ“Œ ì „ëµì  ì‹œì‚¬ì \n")
        f.write("-" * 40 + "\n")
        f.write(f"â€¢ ì„±ì¥ ì¶”ì²œ: {cluster_profiles[best_growth]}\n")
        f.write(f"â€¢ ì£¼ì˜ í•„ìš”: {cluster_profiles[worst_growth]}\n")
    
    print(f"\nâœ… ì¸ì‚¬ì´íŠ¸ ì €ì¥: {OUTPUT_DIR}/business_insights.txt")


def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    """
    print("\n")
    print("â–ˆ" * 80)
    print("â–ˆ" + " " * 78 + "â–ˆ")
    print("â–ˆ" + "  ğŸ“Š ì„œìš¸ì‹œ í•œì‹ìŒì‹ì  ìƒê¶Œ ìœ í˜•í™” ë¶„ì„ (K-means Clustering)".center(76) + "â–ˆ")
    print("â–ˆ" + " " * 78 + "â–ˆ")
    print("â–ˆ" * 80)
    
    # 1. ë°ì´í„° ë¡œë“œ
    df = load_data()
    
    # 2. ì‹œê³„ì—´ íŒŒìƒë³€ìˆ˜ ìƒì„±
    features_df = extract_time_features(df)
    
    # 3. ì „ì²˜ë¦¬
    df_analysis, feature_cols = preprocess_features(features_df)
    
    # 4. í‘œì¤€í™” ë° PCA
    X_scaled, X_pca, pca, scaler, loadings = standardize_and_pca(df_analysis, feature_cols)
    
    # 5-1. ìµœì  k íƒìƒ‰
    optimal_k, silhouettes = find_optimal_k(X_pca, max_k=8)
    
    # 5-2. K-means í´ëŸ¬ìŠ¤í„°ë§
    kmeans, labels = perform_kmeans(X_pca, optimal_k)
    
    # 6. í´ëŸ¬ìŠ¤í„° í•´ì„
    df_result, cluster_stats, cluster_profiles = interpret_clusters(
        df_analysis, feature_cols, labels, 
        ['í‰ê· _ë§¤ì¶œ', 'ë§¤ì¶œ_ì„±ì¥ë¥ ', 'ë§¤ì¶œ_ë³€ë™ì„±', 'ì í¬_ìˆ˜', 'ì´_ìœ ë™ì¸êµ¬_ìˆ˜']
    )
    
    # 7. ì‹œê°í™”
    visualize_clusters(df_result, X_pca, labels, cluster_profiles)
    
    # 8. ê²°ê³¼ ì €ì¥
    save_results(df_result, cluster_stats, cluster_profiles)
    
    # 9. ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸
    generate_business_insights(df_result, cluster_profiles, cluster_stats)
    
    print("\n")
    print("â–ˆ" * 80)
    print("â–ˆ" + " " * 78 + "â–ˆ")
    print("â–ˆ" + "  âœ… ë¶„ì„ ì™„ë£Œ!".center(76) + "â–ˆ")
    print("â–ˆ" + f"  ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {OUTPUT_DIR}/".center(76) + "â–ˆ")
    print("â–ˆ" + " " * 78 + "â–ˆ")
    print("â–ˆ" * 80)
    print("\n")


if __name__ == "__main__":
    main()

